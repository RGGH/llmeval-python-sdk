{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419b1658-fd18-4bc5-aace-8436105ba1c1",
   "metadata": {},
   "source": [
    "# Evaluate Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cafbeb4-211d-4d4f-8b08-0cf1091013cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install \"Evaluate\" - https://github.com/RGGH/evaluate\n",
    "# Configure .env\n",
    "# !pip install llmeval-sdk\n",
    "\n",
    "from llmeval import EvalClient\n",
    "\n",
    "# Initialize the client\n",
    "client = EvalClient(base_url=\"http://127.0.0.1:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2aa0c8-cb68-4f42-95b0-3623cab74c5a",
   "metadata": {},
   "source": [
    "### Check Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cba58a53-bc89-4a94-9e28-ca695f5b6303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'service': 'eval-api', 'status': 'healthy', 'version': '0.1.0'}\n"
     ]
    }
   ],
   "source": [
    "# Check server health\n",
    "status = client.health_check()\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df8386-9f3d-4ad9-a71b-7711ed1c81e7",
   "metadata": {},
   "source": [
    "### Check models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebc4347d-ed60-48b6-bc00-2bfa83300e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['anthropic:claude-opus-4', 'anthropic:claude-sonnet-4-5', 'anthropic:claude-haiku-4', 'gemini:gemini-2.5-pro', 'gemini:gemini-2.5-flash', 'ollama:llama3', 'ollama:gemma', 'openai:gpt-4o', 'openai:gpt-4o-mini', 'openai:gpt-3.5-turbo']\n"
     ]
    }
   ],
   "source": [
    "# Get available models\n",
    "models = client.get_models()\n",
    "print(f\"Available models: {models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f6d8ea-e8c9-417a-8786-0bd96ed26b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'service': 'eval-api', 'status': 'healthy', 'version': '0.1.0'}\n",
      "Available models: ['anthropic:claude-opus-4', 'anthropic:claude-sonnet-4-5', 'anthropic:claude-haiku-4', 'gemini:gemini-2.5-pro', 'gemini:gemini-2.5-flash', 'ollama:llama3', 'ollama:gemma', 'openai:gpt-4o', 'openai:gpt-4o-mini', 'openai:gpt-3.5-turbo']\n",
      "Model output: The capital of France is **Paris**.\n",
      "Judge verdict: Pass\n",
      "Passed: True\n"
     ]
    }
   ],
   "source": [
    "# Run a single evaluation\n",
    "result = client.run_eval(\n",
    "    model=\"gemini:gemini-2.5-pro\",\n",
    "    prompt=\"What is the capital of France?\",\n",
    "    expected=\"Paris\",\n",
    "    judge_model=\"ollama:llama3\"\n",
    ")\n",
    "\n",
    "print(f\"Model output: {result.model_output}\")\n",
    "print(f\"Judge verdict: {result.judge_verdict}\")\n",
    "print(f\"Passed: {result.passed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "035c4175-5a96-45f0-b6f1-1a002a694af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: The capital of France is Paris.\n",
      "Judge verdict: Fail\n",
      "Passed: False\n"
     ]
    }
   ],
   "source": [
    "# Run a single evaluation\n",
    "result = client.run_eval(\n",
    "    model=\"ollama:llama3\",\n",
    "    prompt=\"What is the capital of France?\",\n",
    "    expected=\"Paris\",\n",
    "    judge_model=\"gemini:gemini-2.5-pro\"\n",
    ")\n",
    "\n",
    "print(f\"Model output: {result.model_output}\")\n",
    "print(f\"Judge verdict: {result.judge_verdict}\")\n",
    "print(f\"Passed: {result.passed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e048f13-3e78-4481-907a-3c1fee936f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: The capital of France is Paris.\n",
      "Judge verdict: Pass\n",
      "Passed: True\n"
     ]
    }
   ],
   "source": [
    "# Run a single evaluation\n",
    "result = client.run_eval(\n",
    "    model=\"ollama:llama3\",\n",
    "    prompt=\"What is the capital of France?\",\n",
    "    expected=\"Paris\",\n",
    "    judge_model=\"gemini:gemini-2.5-flash\"\n",
    ")\n",
    "\n",
    "print(f\"Model output: {result.model_output}\")\n",
    "print(f\"Judge verdict: {result.judge_verdict}\")\n",
    "print(f\"Passed: {result.passed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04e92976-546d-4757-9e08-9d161a92f569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 879df5f4-ac82-468b-83a5-533e24586183\n",
      "Status: completed\n",
      "‚úÖ Passed: 2/3\n",
      "üìà Pass rate: 66.67%\n",
      "‚ö° Avg model latency: 2112 ms\n",
      "‚öñÔ∏è Avg judge latency: 7909 ms\n",
      "\n",
      "--- Eval ---\n",
      "Prompt: What is 2 + 2?\n",
      "Model output: 2 + 2 = 4.\n",
      "Expected: 4\n",
      "Verdict: Pass\n",
      "Passed: True\n",
      "Reasoning: Verdict: PASS\n",
      "The core meaning of both outputs is the numerical value '4'. While the actual output provides additional context in the form of an equation, it ultimately arrives at and clearly states '4' as its result. Thus, the fundamental information conveyed‚Äîthe number four‚Äîis consistent between both texts.\n",
      "\n",
      "--- Eval ---\n",
      "Prompt: Translate 'hello' to French\n",
      "Model output: The most common and direct translation of \"hello\" to French is **Bonjour**.\n",
      "\n",
      "You can also use:\n",
      "*   **Salut** (more informal, like \"hi\")\n",
      "Expected: bonjour\n",
      "Verdict: Pass\n",
      "Passed: True\n",
      "Reasoning: Verdict: PASS\n",
      "The actual output contains the expected answer, \"Bonjour,\" and correctly identifies it as the primary translation. While the actual output is more verbose and provides helpful context, it is semantically equivalent because the core information is present and accurate. The additional details enhance the response without changing its fundamental meaning.\n",
      "\n",
      "--- Eval ---\n",
      "Prompt: Who wrote '1984'?\n",
      "Model output: **George Orwell** wrote '1984'.\n",
      "Expected: George Orwell\n",
      "Verdict: Fail\n",
      "Passed: False\n",
      "Reasoning: Verdict: FAIL\n",
      "The actual output is not an exact match to the expected output. It contains additional words (\"wrote '1984'\") and markdown formatting that are absent in the expected text. The evaluation criteria required an exact match, which this fails to meet.\n"
     ]
    }
   ],
   "source": [
    "from llmeval import EvalClient\n",
    "\n",
    "# 1. Initialize the client (connects to your eval server)\n",
    "client = EvalClient(base_url=\"http://localhost:8080\")\n",
    "\n",
    "# 2. Prepare multiple evaluation requests\n",
    "batch_inputs = [\n",
    "    {\n",
    "        \"model\": \"gemini:gemini-2.5-pro\",\n",
    "        \"prompt\": \"What is 2 + 2?\",\n",
    "        \"expected\": \"4\",\n",
    "        \"judge_model\": \"gemini:gemini-2.5-flash\",\n",
    "        #\"criteria\": \"Exact match\" <-- fails if you ask for exact match!\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gemini:gemini-2.5-flash\",\n",
    "        \"prompt\": \"Translate 'hello' to French\",\n",
    "        \"expected\": \"bonjour\",\n",
    "        \"judge_model\": \"gemini:gemini-2.5-pro\",\n",
    "        \"criteria\": \"Semantic equivalence\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gemini:gemini-2.5-flash\",\n",
    "        \"prompt\": \"Who wrote '1984'?\",\n",
    "        \"expected\": \"George Orwell\",\n",
    "        \"judge_model\": \"gemini:gemini-2.5-pro\",\n",
    "        \"criteria\": \"Exact match\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# 3. Run them as a batch\n",
    "batch_result = client.run_batch(batch_inputs)\n",
    "\n",
    "# 4. Inspect summary stats\n",
    "print(f\"\\nüìä Batch {batch_result.batch_id}\")\n",
    "print(f\"Status: {batch_result.status}\")\n",
    "print(f\"‚úÖ Passed: {batch_result.passed}/{batch_result.total}\")\n",
    "print(f\"üìà Pass rate: {batch_result.pass_rate:.2f}%\")\n",
    "print(f\"‚ö° Avg model latency: {batch_result.average_model_latency_ms} ms\")\n",
    "print(f\"‚öñÔ∏è Avg judge latency: {batch_result.average_judge_latency_ms} ms\")\n",
    "\n",
    "# 5. Inspect individual evals\n",
    "for res in batch_result.results:\n",
    "    print(\"\\n--- Eval ---\")\n",
    "    print(f\"Prompt: {res.prompt}\")\n",
    "    print(f\"Model output: {res.model_output}\")\n",
    "    print(f\"Expected: {res.expected}\")\n",
    "    print(f\"Verdict: {res.judge_verdict}\")\n",
    "    print(f\"Passed: {res.passed}\")\n",
    "    if res.judge_result and res.judge_result.reasoning:\n",
    "        print(f\"Reasoning: {res.judge_result.reasoning}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
